/**
 * Reasoning Scenarios Integration Tests
 *
 * Tests the full agent reasoning patterns documented in README.md with real Gemini API.
 * Uses MockMcpClient to simulate canvas operations while testing:
 *
 * 1. Chat handling (no tools) - "hello" â†’ LLM responds naturally
 * 2. Simple tasks (1-2 tools) - "add rectangle" â†’ direct execution
 * 3. Complex tasks with reasoning - uses <analyze>/<plan>/<execute> tags
 * 4. Exploration via tools - canvas_read/find with analysis
 * 5. Planning via reasoning - <plan> tags for multi-step tasks
 * 6. Progress tracking - <execute> tags during execution
 * 7. Delegation - canvas_delegate for specialized sub-agents
 * 8. Clarification - canvas_clarify when task is ambiguous
 *
 * Run with:
 *   GOOGLE_API_KEY=xxx pnpm --filter=@waiboard/ai-agents test src/__tests__/integration/reasoning-scenarios.test.ts
 *
 * @see README.md "Core Design: One Loop, LLM Decides"
 */

import { describe, it, expect, beforeEach, afterEach } from "vitest";
import { EventEmitter } from "node:events";

// Core API (what README documents)
import { streamCanvasAgent, runCanvasAgent, getCanvasAgentResult } from "../../core/consumption.js";
import { runAgenticLoop } from "../../core/loop.js";
import type { AgentConfig, AgentEvent } from "../../agents/types.js";

// Hooks for observability
import { createHooksManager, type HooksManager } from "../../hooks/manager.js";

// Context for integration testing
import { createContextManager, type ContextManager } from "../../context/manager.js";

// Response extraction for verifying reasoning tags
import { extractReasoning, parseLLMOutput, type ParsedLLMOutput } from "../../core/response-extractor.js";

// ============================================================================
// TEST CONFIGURATION
// ============================================================================

const RUN_LIVE_TESTS = process.env.GOOGLE_API_KEY !== undefined;
const describeIfLive = RUN_LIVE_TESTS ? describe : describe.skip;

// Test timeouts (Gemini can take a while)
const TIMEOUT_CHAT = 30_000; // 30s for chat (simple response)
const TIMEOUT_SIMPLE = 45_000; // 45s for simple tasks
const TIMEOUT_COMPLEX = 90_000; // 90s for complex multi-step
const TIMEOUT_DELEGATION = 120_000; // 120s for delegation scenarios

// ============================================================================
// MOCK CANVAS DRIVER (Realistic Implementation)
// ============================================================================

interface CanvasElement {
  id: string;
  type: string;
  x: number;
  y: number;
  width?: number;
  height?: number;
  backgroundColor?: string;
  strokeColor?: string;
  strokeWidth?: number;
  text?: string;
  name?: string;
  parentId?: string;
}

class MockCanvasDriver extends EventEmitter {
  private elements: Map<string, CanvasElement> = new Map();
  private idCounter = 1;
  public toolCallLog: Array<{ tool: string; args: unknown; timestamp: number }> = [];

  generateId(): string {
    return `elem_${this.idCounter++}`;
  }

  create(spec: Omit<CanvasElement, "id">): CanvasElement {
    const id = this.generateId();
    const element: CanvasElement = { id, ...spec };
    this.elements.set(id, element);
    this.emit("element:created", element);
    return element;
  }

  update(id: string, changes: Partial<CanvasElement>): boolean {
    const element = this.elements.get(id);
    if (!element) return false;
    Object.assign(element, changes);
    return true;
  }

  delete(id: string): boolean {
    return this.elements.delete(id);
  }

  getById(id: string): CanvasElement | undefined {
    return this.elements.get(id);
  }

  getAll(): CanvasElement[] {
    return Array.from(this.elements.values());
  }

  count(): number {
    return this.elements.size;
  }

  clear(): void {
    this.elements.clear();
  }

  findByType(type: string): CanvasElement[] {
    return this.getAll().filter((el) => el.type === type);
  }

  findByName(pattern: string | RegExp): CanvasElement[] {
    const regex = typeof pattern === "string" ? new RegExp(pattern, "i") : pattern;
    return this.getAll().filter((el) => el.name && regex.test(el.name));
  }

  resetToolCallLog(): void {
    this.toolCallLog = [];
  }
}

// ============================================================================
// MOCK MCP CLIENT (Simulates canvas-mcp server)
// ============================================================================

class MockMcpClient {
  private driver: MockCanvasDriver;
  public callCount = 0;
  private connected = true;

  constructor(driver: MockCanvasDriver) {
    this.driver = driver;
  }

  async connect(): Promise<boolean> {
    return this.connected;
  }

  disconnect(): void {
    this.connected = false;
  }

  isConnected(): boolean {
    return this.connected;
  }

  async execute<T>(toolName: string, args: Record<string, unknown>): Promise<T> {
    this.callCount++;
    this.driver.toolCallLog.push({ tool: toolName, args, timestamp: Date.now() });

    switch (toolName) {
      case "canvas_read": {
        const target = args.target as string;
        if (target === "canvas") {
          const elements = this.driver.getAll();
          const typeCounts: Record<string, number> = {};
          elements.forEach((el) => {
            typeCounts[el.type] = (typeCounts[el.type] || 0) + 1;
          });
          return {
            count: elements.length,
            types: typeCounts,
            elements: elements.map((e) => ({ id: e.id, type: e.type, name: e.name })),
          } as T;
        }
        if (target === "selection" || target === "viewport") {
          return { elements: [], count: 0 } as T;
        }
        const element = this.driver.getById(target);
        return { element: element || null, found: !!element } as T;
      }

      case "canvas_write": {
        const elementsToCreate = args.elements as Array<Record<string, unknown>>;
        const clearFirst = args.clearFirst as boolean;
        if (clearFirst) this.driver.clear();

        const createdIds: string[] = [];
        for (const spec of elementsToCreate || []) {
          const element = this.driver.create({
            type: spec.type as string,
            x: (spec.x as number) || 0,
            y: (spec.y as number) || 0,
            width: spec.width as number | undefined,
            height: spec.height as number | undefined,
            backgroundColor: spec.backgroundColor as string | undefined,
            text: spec.text as string | undefined,
            name: spec.name as string | undefined,
          });
          createdIds.push(element.id);
        }
        return { success: true, createdIds, count: createdIds.length } as T;
      }

      case "canvas_edit": {
        const operation = args.operation as string;
        const target = args.target as string;
        switch (operation) {
          case "delete":
            return { success: this.driver.delete(target) } as T;
          case "update":
          case "style": {
            const properties = (args.properties as Record<string, unknown>) || {};
            return { success: this.driver.update(target, properties as Partial<CanvasElement>) } as T;
          }
          case "move": {
            const delta = args.delta as { x?: number; y?: number } | undefined;
            const element = this.driver.getById(target);
            if (element) {
              this.driver.update(target, {
                x: element.x + (delta?.x ?? 0),
                y: element.y + (delta?.y ?? 0),
              });
              return { success: true } as T;
            }
            return { success: false } as T;
          }
          default:
            return { success: false, error: `Unknown operation: ${operation}` } as T;
        }
      }

      case "canvas_find": {
        const match = (args.match || {}) as Record<string, unknown>;
        let results = this.driver.getAll();

        if (match.type) {
          results = results.filter((el) => el.type === match.type);
        }
        if (match.name) {
          const regex = new RegExp(match.name as string, "i");
          results = results.filter((el) => el.name && regex.test(el.name));
        }
        if (match.text) {
          const regex = new RegExp(match.text as string, "i");
          results = results.filter((el) => el.text && regex.test(el.text));
        }

        return {
          count: results.length,
          elements: results.map((el) => ({ id: el.id, type: el.type, name: el.name })),
        } as T;
      }

      case "canvas_capture": {
        return {
          success: true,
          format: "base64",
          data: "data:image/png;base64,iVBORw0KGgoAAAANSUhEUg...",
          width: 800,
          height: 600,
        } as T;
      }

      case "canvas_layout": {
        const targetIds = args.target as string[];
        return {
          success: true,
          layoutType: args.type,
          affectedCount: targetIds?.length ?? 0,
        } as T;
      }

      case "canvas_style": {
        const targetIds = args.target as string[];
        let styled = 0;
        if (targetIds) {
          const rules = (args.rules || {}) as Record<string, unknown>;
          for (const id of targetIds) {
            if (this.driver.update(id, rules as Partial<CanvasElement>)) {
              styled++;
            }
          }
        }
        return { success: true, styledCount: styled } as T;
      }

      case "canvas_delegate": {
        // Simulate delegation result
        return {
          success: true,
          summary: `Delegated task completed: ${args.task}`,
          createdIds: [],
          modifiedIds: [],
          tokensUsed: 500,
        } as T;
      }

      case "canvas_clarify": {
        // Return interrupt result
        return {
          interrupt: true,
          clarificationId: `clarify_${Date.now()}`,
          questions: args.questions || [],
          reason: args.reason || "Need clarification",
          options: args.options || [],
          clarificationType: args.clarificationType || "general",
        } as T;
      }

      case "canvas_snapshot": {
        const elements = this.driver.getAll();
        const typeCounts: Record<string, number> = {};
        elements.forEach((el) => {
          typeCounts[el.type] = (typeCounts[el.type] || 0) + 1;
        });
        return {
          elementCount: elements.length,
          typeCounts,
          selection: [],
          viewport: { x: 0, y: 0, zoom: 1 },
          summary: `Canvas with ${elements.length} elements`,
          tokensUsed: 50,
        } as T;
      }

      default:
        throw new Error(`Unknown tool: ${toolName}`);
    }
  }

  async readResource<T>(_uri: string): Promise<T> {
    const elements = this.driver.getAll();
    const typeCounts: Record<string, number> = {};
    elements.forEach((el) => {
      typeCounts[el.type] = (typeCounts[el.type] || 0) + 1;
    });
    return {
      elementCount: elements.length,
      typeCounts,
      selection: [],
      viewport: { x: 0, y: 0, zoom: 1 },
      summary: `Canvas with ${elements.length} elements`,
      skeleton: elements.map((e) => ({ id: e.id, type: e.type })),
      relevant: [],
      tokensUsed: 100,
    } as T;
  }

  getDriver(): MockCanvasDriver {
    return this.driver;
  }
}

// ============================================================================
// TEST UTILITIES
// ============================================================================

function createTestConfig(driver: MockCanvasDriver): AgentConfig {
  return {
    canvasId: "test-canvas",
    sessionId: `test-session-${Date.now()}`,
    verbose: false,
    maxIterations: 10,
    skipMcpConnection: false,
    mcpUrl: "http://localhost:3112", // Will be overridden by mock
  };
}

function collectEvents(events: AgentEvent[]): {
  thinking: number;
  acting: number;
  observing: number;
  complete: boolean;
  failed: boolean;
  clarificationNeeded: boolean;
  tools: string[];
  reasoning: string[];
} {
  const tools: string[] = [];
  const reasoning: string[] = [];

  for (const event of events) {
    if (event.type === "acting" && event.tool) {
      tools.push(event.tool as string);
    }
    if (event.type === "reasoning" && event.content) {
      reasoning.push(event.content);
    }
  }

  return {
    thinking: events.filter((e) => e.type === "thinking").length,
    acting: events.filter((e) => e.type === "acting").length,
    observing: events.filter((e) => e.type === "observing").length,
    complete: events.some((e) => e.type === "complete"),
    failed: events.some((e) => e.type === "failed"),
    clarificationNeeded: events.some((e) => e.type === "clarification_needed"),
    tools,
    reasoning,
  };
}

function hasReasoningTag(events: AgentEvent[], tag: "analyze" | "plan" | "execute"): boolean {
  for (const event of events) {
    if (event.type === "reasoning" && event.content) {
      const parsed = parseLLMOutput(event.content);
      switch (tag) {
        case "analyze":
          if (parsed.analyze) return true;
          break;
        case "plan":
          if (parsed.plan) return true;
          break;
        case "execute":
          if (parsed.execute) return true;
          break;
      }
    }
  }
  return false;
}

function logScenarioResult(
  scenario: string,
  events: AgentEvent[],
  driver: MockCanvasDriver
): void {
  const summary = collectEvents(events);
  console.log(`\nğŸ“Š ${scenario}:`);
  console.log(`   Thinking: ${summary.thinking}, Acting: ${summary.acting}, Observing: ${summary.observing}`);
  console.log(`   Tools: ${summary.tools.join(", ") || "none"}`);
  console.log(`   Elements created: ${driver.count()}`);
  console.log(`   Complete: ${summary.complete}, Failed: ${summary.failed}, Clarification: ${summary.clarificationNeeded}`);
}

// ============================================================================
// SCENARIO TESTS: README.md Reasoning Patterns
// ============================================================================

describeIfLive("Reasoning Scenarios (Real Gemini API)", () => {
  let driver: MockCanvasDriver;
  let mcpClient: MockMcpClient;
  let hooks: HooksManager;

  beforeEach(() => {
    driver = new MockCanvasDriver();
    mcpClient = new MockMcpClient(driver);
    hooks = createHooksManager();
  });

  afterEach(() => {
    driver.resetToolCallLog();
    hooks.destroy();
  });

  // ==========================================================================
  // SCENARIO 1: Chat Handling (No Tools)
  // README: "Chat" ("hello") â†’ LLM responds directly, no tools
  // Note: Tests with real LLM verify chat intent detection
  // ==========================================================================
  describe("Scenario 1: Chat Handling (No Tools)", () => {
    it("should respond to 'hello' without using any tools", async () => {
      const events: AgentEvent[] = [];

      // Use MockMcpClient for tool execution while using real Gemini API
      for await (const event of runAgenticLoop("hello", "test-canvas", {
        verbose: false,
        hooksManager: hooks,
        mcpClient: mcpClient as any, // Cast to satisfy type checker
      })) {
        events.push(event);
      }

      const summary = collectEvents(events);

      // Chat should terminate - LLM decides whether to use tools or not
      // Valid terminal states: complete (responded), failed (error), or clarificationNeeded (asking questions)
      expect(summary.complete || summary.failed || summary.clarificationNeeded).toBe(true);
      // For pure chat, expect no tools (but LLM may vary)
      // This is a behavioral test - the README says chat should not use tools

      logScenarioResult("Chat Handling", events, driver);
    }, TIMEOUT_CHAT);

    it("should respond to 'thanks for your help' without tools", async () => {
      const events: AgentEvent[] = [];

      for await (const event of runAgenticLoop("thanks for your help", "test-canvas", {
        verbose: false,
        hooksManager: hooks,
        mcpClient: mcpClient as any,
      })) {
        events.push(event);
      }

      const summary = collectEvents(events);

      // Should terminate (complete, fail, or ask clarification)
      expect(summary.complete || summary.failed || summary.clarificationNeeded).toBe(true);
    }, TIMEOUT_CHAT);

    it("should answer questions about capabilities without tools", async () => {
      const events: AgentEvent[] = [];

      for await (const event of runAgenticLoop(
        "what can you help me with?",
        "test-canvas",
        {
          verbose: false,
          hooksManager: hooks,
          mcpClient: mcpClient as any,
        }
      )) {
        events.push(event);
      }

      const summary = collectEvents(events);

      expect(summary.complete || summary.failed).toBe(true);
      // Should respond conversationally - LLM decides tool usage
    }, TIMEOUT_CHAT);
  });

  // ==========================================================================
  // SCENARIO 2: Simple Tasks (1-2 Tools)
  // README: "Simple tasks" ("add rectangle") â†’ LLM uses 1-2 tools
  // Note: Real LLM tests verify the agent makes appropriate tool calls
  // ==========================================================================
  describe("Scenario 2: Simple Tasks (1-2 Tools)", () => {
    it("should handle simple rectangle creation request", async () => {
      const events: AgentEvent[] = [];

      for await (const event of runAgenticLoop(
        "add a blue rectangle at position 100,100",
        "test-canvas",
        {
          verbose: false,
          hooksManager: hooks,
          mcpClient: mcpClient as any,
        }
      )) {
        events.push(event);
      }

      const summary = collectEvents(events);

      // Test should terminate (success or failure)
      expect(summary.complete || summary.failed).toBe(true);
      // If successful and tools were used, verify reasonable tool usage
      if (summary.complete && summary.tools.length > 0) {
        expect(summary.tools.length).toBeLessThanOrEqual(5);
      }

      logScenarioResult("Simple Task - Add Rectangle", events, driver);
    }, TIMEOUT_SIMPLE);

    it("should handle element deletion request", async () => {
      const events: AgentEvent[] = [];

      for await (const event of runAgenticLoop(
        "delete the element named 'to-delete'",
        "test-canvas",
        {
          verbose: false,
          hooksManager: hooks,
          mcpClient: mcpClient as any,
        }
      )) {
        events.push(event);
      }

      const summary = collectEvents(events);

      // Test should terminate
      expect(summary.complete || summary.failed).toBe(true);
    }, TIMEOUT_SIMPLE);
  });

  // ==========================================================================
  // SCENARIO 3: Complex Tasks with Reasoning
  // README: "Complex tasks" â†’ LLM iterates with <analyze>/<plan>/<execute>
  // Note: Requires running MCP server for full tool execution
  // ==========================================================================
  describe("Scenario 3: Complex Tasks with Reasoning", () => {
    it("should handle canvas analysis request", async () => {
      const events: AgentEvent[] = [];

      for await (const event of runAgenticLoop(
        "analyze the current canvas and tell me what elements are there",
        "test-canvas",
        {
          verbose: false,
          hooksManager: hooks,
          mcpClient: mcpClient as any,
        }
      )) {
        events.push(event);
      }

      const summary = collectEvents(events);

      // Should terminate
      expect(summary.complete || summary.failed).toBe(true);
      // If successful with MCP, should use canvas_read for exploration
      if (summary.complete && summary.tools.length > 0) {
        expect(summary.tools.some(t => t === "canvas_read" || t === "canvas_find")).toBe(true);
      }

      logScenarioResult("Complex - Analyze Canvas", events, driver);
    }, TIMEOUT_COMPLEX);

    it("should handle multi-step flowchart creation", async () => {
      const events: AgentEvent[] = [];

      for await (const event of runAgenticLoop(
        "create a simple flowchart with 3 steps: Start â†’ Process â†’ End",
        "test-canvas",
        {
          verbose: false,
          hooksManager: hooks,
          mcpClient: mcpClient as any,
          maxIterations: 15,
        }
      )) {
        events.push(event);
      }

      const summary = collectEvents(events);

      // Should terminate
      expect(summary.complete || summary.failed).toBe(true);
      // If successful, complex task should use write operations
      if (summary.complete && summary.tools.length > 0) {
        expect(summary.tools.some(t => t === "canvas_write")).toBe(true);
      }

      logScenarioResult("Complex - Flowchart Creation", events, driver);
    }, TIMEOUT_COMPLEX);

    it("should handle grid layout creation", async () => {
      const events: AgentEvent[] = [];

      for await (const event of runAgenticLoop(
        "create 4 colored rectangles (red, green, blue, yellow) arranged in a 2x2 grid",
        "test-canvas",
        {
          verbose: false,
          hooksManager: hooks,
          mcpClient: mcpClient as any,
          maxIterations: 15,
        }
      )) {
        events.push(event);
      }

      const summary = collectEvents(events);

      // Should terminate
      expect(summary.complete || summary.failed).toBe(true);

      logScenarioResult("Complex - Grid Layout", events, driver);
    }, TIMEOUT_COMPLEX);
  });

  // ==========================================================================
  // SCENARIO 4: Exploration via Tools
  // README: Exploration - LLM uses canvas_read, canvas_find tools
  // ==========================================================================
  describe("Scenario 4: Exploration via Tools", () => {
    it("should handle canvas state query", async () => {
      const events: AgentEvent[] = [];

      for await (const event of runAgenticLoop(
        "what's currently on the canvas?",
        "test-canvas",
        {
          verbose: false,
          hooksManager: hooks,
          mcpClient: mcpClient as any,
        }
      )) {
        events.push(event);
      }

      const summary = collectEvents(events);

      // Should terminate
      expect(summary.complete || summary.failed).toBe(true);
      // If successful, should use canvas_read for exploration
      if (summary.complete && summary.tools.length > 0) {
        expect(summary.tools.some(t => t === "canvas_read" || t === "canvas_find")).toBe(true);
      }
    }, TIMEOUT_SIMPLE);

    it("should handle element search request", async () => {
      const events: AgentEvent[] = [];

      for await (const event of runAgenticLoop(
        "find all rectangles on the canvas",
        "test-canvas",
        {
          verbose: false,
          hooksManager: hooks,
          mcpClient: mcpClient as any,
        }
      )) {
        events.push(event);
      }

      const summary = collectEvents(events);

      // Should terminate
      expect(summary.complete || summary.failed).toBe(true);
      // If successful, should use search tools
      if (summary.complete && summary.tools.length > 0) {
        expect(summary.tools.some((t) => t === "canvas_find" || t === "canvas_read")).toBe(true);
      }
    }, TIMEOUT_SIMPLE);
  });

  // ==========================================================================
  // SCENARIO 5: Read â†’ Plan â†’ Execute â†’ Verify Pattern
  // README: Multi-step reasoning chain
  // ==========================================================================
  describe("Scenario 5: Read â†’ Plan â†’ Execute â†’ Verify", () => {
    it("should handle multi-step reasoning request", async () => {
      const events: AgentEvent[] = [];

      for await (const event of runAgenticLoop(
        "look at the canvas, then add a text label 'Title' above any existing rectangle, and verify it was created",
        "test-canvas",
        {
          verbose: false,
          hooksManager: hooks,
          mcpClient: mcpClient as any,
          maxIterations: 10,
        }
      )) {
        events.push(event);
      }

      const summary = collectEvents(events);

      // Should terminate
      expect(summary.complete || summary.failed).toBe(true);
      // If successful, should use both read and write operations
      if (summary.complete && summary.tools.length >= 2) {
        expect(summary.tools.some(t => t === "canvas_read")).toBe(true);
        expect(summary.tools.some(t => t === "canvas_write")).toBe(true);
      }

      logScenarioResult("Read â†’ Plan â†’ Execute â†’ Verify", events, driver);
    }, TIMEOUT_COMPLEX);
  });

  // ==========================================================================
  // SCENARIO 6: Error Recovery
  // README: LLM should handle errors gracefully
  // ==========================================================================
  describe("Scenario 6: Error Recovery", () => {
    it("should handle missing element gracefully", async () => {
      const events: AgentEvent[] = [];

      for await (const event of runAgenticLoop(
        "delete the element with id 'nonexistent-12345'",
        "test-canvas",
        {
          verbose: false,
          hooksManager: hooks,
          mcpClient: mcpClient as any,
        }
      )) {
        events.push(event);
      }

      const summary = collectEvents(events);

      // Should terminate (either complete or fail gracefully)
      expect(summary.complete || summary.failed).toBe(true);
    }, TIMEOUT_SIMPLE);

    it("should handle empty search results gracefully", async () => {
      const events: AgentEvent[] = [];

      for await (const event of runAgenticLoop(
        "find all purple unicorns and change their color",
        "test-canvas",
        {
          verbose: false,
          hooksManager: hooks,
          mcpClient: mcpClient as any,
        }
      )) {
        events.push(event);
      }

      const summary = collectEvents(events);

      // Should terminate gracefully
      expect(summary.complete || summary.failed).toBe(true);
    }, TIMEOUT_SIMPLE);
  });

  // ==========================================================================
  // SCENARIO 7: Conditional Logic
  // README: LLM takes appropriate action based on canvas state
  // ==========================================================================
  describe("Scenario 7: Conditional Logic", () => {
    it("should handle conditional request on empty canvas", async () => {
      const events: AgentEvent[] = [];

      for await (const event of runAgenticLoop(
        "if the canvas is empty, create a welcome message. Otherwise, list what's there.",
        "test-canvas",
        {
          verbose: false,
          hooksManager: hooks,
          mcpClient: mcpClient as any,
        }
      )) {
        events.push(event);
      }

      const summary = collectEvents(events);

      // Should terminate
      expect(summary.complete || summary.failed).toBe(true);
      // If successful, should read canvas to check state
      if (summary.complete && summary.tools.length > 0) {
        expect(summary.tools.some(t => t === "canvas_read")).toBe(true);
      }

      logScenarioResult("Conditional - Empty Canvas", events, driver);
    }, TIMEOUT_SIMPLE);

    it("should handle conditional request", async () => {
      const events: AgentEvent[] = [];

      for await (const event of runAgenticLoop(
        "if the canvas is empty, create a welcome message. Otherwise, describe what's there.",
        "test-canvas",
        {
          verbose: false,
          hooksManager: hooks,
          mcpClient: mcpClient as any,
        }
      )) {
        events.push(event);
      }

      const summary = collectEvents(events);

      // Should terminate
      expect(summary.complete || summary.failed).toBe(true);

      logScenarioResult("Conditional - Non-Empty Canvas", events, driver);
    }, TIMEOUT_SIMPLE);
  });

  // ==========================================================================
  // SCENARIO 8: Batch Operations
  // README: LLM should use efficient batch operations
  // ==========================================================================
  describe("Scenario 8: Batch Operations", () => {
    it("should handle batch element creation request", async () => {
      const events: AgentEvent[] = [];

      for await (const event of runAgenticLoop(
        "create 3 rectangles in a row: red, green, blue. Each 50x50, spaced 10px apart starting at (0,0). Use a single write operation.",
        "test-canvas",
        {
          verbose: false,
          hooksManager: hooks,
          mcpClient: mcpClient as any,
        }
      )) {
        events.push(event);
      }

      const summary = collectEvents(events);

      // Should terminate
      expect(summary.complete || summary.failed).toBe(true);
      // If successful, should use write call(s)
      if (summary.complete && summary.tools.length > 0) {
        expect(summary.tools.some(t => t === "canvas_write")).toBe(true);
      }

      logScenarioResult("Batch - Multiple Elements", events, driver);
    }, TIMEOUT_SIMPLE);
  });
});

// ============================================================================
// SCENARIO 9: Delegation via canvas_delegate
// README: LLM uses canvas_delegate tool for specialized sub-agents
// ============================================================================

describeIfLive("Scenario 9: Delegation", () => {
  let driver: MockCanvasDriver;
  let mcpClient: MockMcpClient;
  let hooks: HooksManager;

  beforeEach(() => {
    driver = new MockCanvasDriver();
    mcpClient = new MockMcpClient(driver);
    hooks = createHooksManager();
  });

  afterEach(() => {
    driver.resetToolCallLog();
    hooks.destroy();
  });

  it("should delegate complex diagram creation to specialist", async () => {
    const events: AgentEvent[] = [];

    for await (const event of runAgenticLoop(
      "create a complex mind map about artificial intelligence with branches for machine learning, natural language processing, computer vision, and robotics",
      "test-canvas",
      {
        verbose: false,
        hooksManager: hooks,
        mcpClient: mcpClient as any,
        maxIterations: 20,
      }
    )) {
      events.push(event);
    }

    const summary = collectEvents(events);

    // Should terminate
    expect(summary.complete || summary.failed).toBe(true);
    // May use delegation for complex tasks
    // Note: LLM decides whether to delegate or handle directly

    logScenarioResult("Delegation - Mind Map", events, driver);
  }, TIMEOUT_DELEGATION);

  it("should handle delegation for design tasks", async () => {
    const events: AgentEvent[] = [];

    for await (const event of runAgenticLoop(
      "design a professional infographic layout with a header, 3 data cards, and a footer section",
      "test-canvas",
      {
        verbose: false,
        hooksManager: hooks,
        mcpClient: mcpClient as any,
        maxIterations: 20,
      }
    )) {
      events.push(event);
    }

    const summary = collectEvents(events);

    // Should terminate
    expect(summary.complete || summary.failed).toBe(true);
    // If successful with tools, may use delegation or direct execution
    if (summary.complete && summary.tools.length > 0) {
      // Either delegates or writes directly
      expect(
        summary.tools.some((t) => t === "canvas_delegate" || t === "canvas_write")
      ).toBe(true);
    }

    logScenarioResult("Delegation - Infographic", events, driver);
  }, TIMEOUT_DELEGATION);
}, TIMEOUT_DELEGATION);

// ============================================================================
// SCENARIO 10: Clarification via canvas_clarify
// README: LLM asks for clarification when task is ambiguous
// ============================================================================

describeIfLive("Scenario 10: Clarification", () => {
  let driver: MockCanvasDriver;
  let mcpClient: MockMcpClient;
  let hooks: HooksManager;

  beforeEach(() => {
    driver = new MockCanvasDriver();
    mcpClient = new MockMcpClient(driver);
    hooks = createHooksManager();
  });

  afterEach(() => {
    driver.resetToolCallLog();
    hooks.destroy();
  });

  it("should ask for clarification on ambiguous requests", async () => {
    const events: AgentEvent[] = [];

    for await (const event of runAgenticLoop(
      "help me visualize something",
      "test-canvas",
      {
        verbose: false,
        hooksManager: hooks,
        mcpClient: mcpClient as any,
      }
    )) {
      events.push(event);
    }

    const summary = collectEvents(events);

    // Should terminate - clarification_needed is the expected behavior for ambiguous requests
    // but complete/failed are also valid if LLM decides to respond naturally
    expect(summary.complete || summary.failed || summary.clarificationNeeded).toBe(true);
    // LLM may use canvas_clarify or respond naturally asking questions
    // Both are valid behaviors for ambiguous requests

    logScenarioResult("Clarification - Ambiguous Request", events, driver);
  }, TIMEOUT_SIMPLE);

  it("should not ask for clarification on clear requests", async () => {
    const events: AgentEvent[] = [];

    for await (const event of runAgenticLoop(
      "create a blue rectangle at position 100,100 with width 200 and height 100",
      "test-canvas",
      {
        verbose: false,
        hooksManager: hooks,
        mcpClient: mcpClient as any,
      }
    )) {
      events.push(event);
    }

    const summary = collectEvents(events);

    // Should terminate
    expect(summary.complete || summary.failed).toBe(true);
    // Clear request should not need clarification
    if (summary.complete) {
      expect(events.some((e) => e.type === "clarification_needed")).toBe(false);
    }
  }, TIMEOUT_SIMPLE);
}, TIMEOUT_SIMPLE);

// ============================================================================
// REASONING TAG EXTRACTION TESTS (Unit Tests - Always Run)
// ============================================================================

describe("Reasoning Tag Extraction", () => {
  it("should extract <analyze> tag content", () => {
    const text = `<analyze>Complex task, need to understand canvas first</analyze>

Let me check the canvas state.`;

    const parsed = parseLLMOutput(text);

    expect(parsed.reasoning.analyze).toBe("Complex task, need to understand canvas first");
  });

  it("should extract <plan> tag content", () => {
    const text = `<plan>
1. Create header frame with title
2. Delegate timeline to diagram-expert
3. Create statistics section with 4 cards
4. Add footer with call-to-action
</plan>

Starting with step 1...`;

    const parsed = parseLLMOutput(text);

    expect(parsed.reasoning.plan).toContain("Create header frame with title");
    expect(parsed.reasoning.plan).toContain("Delegate timeline");
  });

  it("should extract <summarize> tag content for progress", () => {
    const text = `<summarize>
âœ“ Header created (frame-header)
â†’ Delegating timeline to diagram-expert...
</summarize>`;

    const parsed = parseLLMOutput(text);

    expect(parsed.reasoning.summarize).toContain("Header created");
  });

  it("should extract <observe> tag content for tool results", () => {
    const text = `<observe>
Tool returned: 3 rectangles found with ids elem_1, elem_2, elem_3
</observe>

I found 3 rectangles on your canvas.`;

    const parsed = parseLLMOutput(text);

    expect(parsed.reasoning.observe).toContain("3 rectangles found");
  });

  it("should extract <decide> tag content for decisions", () => {
    const text = `<decide>
Since the user asked for a flowchart, I'll use canvas_write to create the nodes.
</decide>

Creating your flowchart now...`;

    const parsed = parseLLMOutput(text);

    expect(parsed.reasoning.decide).toContain("flowchart");
  });

  it("should extract user-facing response without reasoning tags", () => {
    const text = `<analyze>Need to check canvas</analyze>

<plan>1. Read canvas 2. Create element</plan>

I've created a blue rectangle at position 100,100.`;

    const parsed = parseLLMOutput(text);
    const userResponse = parsed.userResponse;

    expect(userResponse).not.toContain("<analyze>");
    expect(userResponse).not.toContain("<plan>");
    expect(userResponse).toContain("blue rectangle");
  });

  it("should handle text without reasoning tags", () => {
    const text = "Hello! How can I help you with the canvas today?";

    const parsed = parseLLMOutput(text);

    expect(parsed.reasoning.analyze).toBeNull();
    expect(parsed.reasoning.plan).toBeNull();
    expect(parsed.userResponse).toBe(text.trim());
    expect(parsed.hasUserContent).toBe(true);
  });

  it("should identify reasoning-only output", () => {
    const text = `<analyze>
This is a complex task that requires careful planning.
</analyze>

<plan>
1. First step
2. Second step
</plan>`;

    const parsed = parseLLMOutput(text);

    // Only whitespace outside tags
    expect(parsed.hasUserContent).toBe(false);
  });

  it("should preserve user content with mixed reasoning", () => {
    const text = `<analyze>Checking what user wants</analyze>

Here's what I'll do for you:

<plan>Create 3 elements</plan>

1. First, I'll add a header
2. Then, I'll add the content area
3. Finally, I'll add a footer

<summarize>Created outline</summarize>`;

    const parsed = parseLLMOutput(text);

    expect(parsed.userResponse).toContain("Here's what I'll do for you:");
    expect(parsed.userResponse).toContain("1. First, I'll add a header");
    expect(parsed.userResponse).not.toContain("<analyze>");
    expect(parsed.hasUserContent).toBe(true);
  });
});

// ============================================================================
// SKIP INFO
// ============================================================================

describe("Reasoning Scenarios Tests (Skip Info)", () => {
  it("should print skip message when tests are not enabled", () => {
    if (!RUN_LIVE_TESTS) {
      console.log(`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Reasoning Scenarios Tests Skipped                                      â”‚
â”‚                                                                         â”‚
â”‚  These tests validate the reasoning patterns from README.md with        â”‚
â”‚  real Gemini API calls.                                                 â”‚
â”‚                                                                         â”‚
â”‚  To run:                                                                â”‚
â”‚  GOOGLE_API_KEY=xxx pnpm --filter=@waiboard/ai-agents test \\            â”‚
â”‚    src/__tests__/integration/reasoning-scenarios.test.ts                â”‚
â”‚                                                                         â”‚
â”‚  Estimated cost: ~$0.05-0.20 per full run                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      `);
    }
    expect(true).toBe(true);
  });
});
